{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ensemble",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "odkz6q4ACC0p"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHjAp6xpCJBJ"
      },
      "source": [
        "df = pd.read_csv(\"results.csv\")"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "WCrp3-5oCs9i",
        "outputId": "d1514151-a5e4-414f-93a3-440ffdea553c"
      },
      "source": [
        "df"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>INDICATOR</th>\n",
              "      <th>Unemployment actual</th>\n",
              "      <th>INDICATOR.1</th>\n",
              "      <th>RealGDP actual</th>\n",
              "      <th>INDICATOR.2</th>\n",
              "      <th>Core CPI actual</th>\n",
              "      <th>INDICATOR.3</th>\n",
              "      <th>Core PCE actual</th>\n",
              "      <th>XGBoost Unemployment</th>\n",
              "      <th>XGBoost RealGDP</th>\n",
              "      <th>XGBoost Core CPI</th>\n",
              "      <th>XGBoost Core PCE</th>\n",
              "      <th>Random Forest Unemployment</th>\n",
              "      <th>Random Forest RealGDP</th>\n",
              "      <th>Random Forest Core CPI</th>\n",
              "      <th>Random Forest Core PCE</th>\n",
              "      <th>Linear Regression Unemployment</th>\n",
              "      <th>Linear Regression RealGDP</th>\n",
              "      <th>Linear Regression Core CPI</th>\n",
              "      <th>Linear Regression Core PCE</th>\n",
              "      <th>KNeigborsRegressor Unemployment</th>\n",
              "      <th>KNeigborsRegressor RealGDP</th>\n",
              "      <th>KNeigborsRegressor Core CPI</th>\n",
              "      <th>KNeigborsRegressor Core PCE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Unemployment</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>RealGDP</td>\n",
              "      <td>2.99646</td>\n",
              "      <td>Core CPI</td>\n",
              "      <td>2.290573</td>\n",
              "      <td>Core PCE</td>\n",
              "      <td>1.184167</td>\n",
              "      <td>0.081926</td>\n",
              "      <td>2.500126</td>\n",
              "      <td>2.291859</td>\n",
              "      <td>1.232153</td>\n",
              "      <td>0.081083</td>\n",
              "      <td>2.609857</td>\n",
              "      <td>2.279672</td>\n",
              "      <td>1.250697</td>\n",
              "      <td>2.763758</td>\n",
              "      <td>1.913119</td>\n",
              "      <td>2.037117</td>\n",
              "      <td>2.153685</td>\n",
              "      <td>1.621667</td>\n",
              "      <td>2.563892</td>\n",
              "      <td>2.270754</td>\n",
              "      <td>1.237391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Unemployment</td>\n",
              "      <td>3.891667</td>\n",
              "      <td>RealGDP</td>\n",
              "      <td>4.44722</td>\n",
              "      <td>Core CPI</td>\n",
              "      <td>2.174324</td>\n",
              "      <td>Core PCE</td>\n",
              "      <td>1.184167</td>\n",
              "      <td>3.820591</td>\n",
              "      <td>4.123776</td>\n",
              "      <td>2.132030</td>\n",
              "      <td>1.245136</td>\n",
              "      <td>3.866667</td>\n",
              "      <td>4.211068</td>\n",
              "      <td>2.174324</td>\n",
              "      <td>1.404765</td>\n",
              "      <td>4.956422</td>\n",
              "      <td>2.258982</td>\n",
              "      <td>1.673650</td>\n",
              "      <td>1.083818</td>\n",
              "      <td>3.850000</td>\n",
              "      <td>4.319114</td>\n",
              "      <td>2.174324</td>\n",
              "      <td>1.439257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Unemployment</td>\n",
              "      <td>3.683333</td>\n",
              "      <td>RealGDP</td>\n",
              "      <td>4.75324</td>\n",
              "      <td>Core CPI</td>\n",
              "      <td>2.174324</td>\n",
              "      <td>Core PCE</td>\n",
              "      <td>1.647664</td>\n",
              "      <td>3.852198</td>\n",
              "      <td>4.178174</td>\n",
              "      <td>2.079864</td>\n",
              "      <td>1.627884</td>\n",
              "      <td>3.716916</td>\n",
              "      <td>4.146253</td>\n",
              "      <td>1.940770</td>\n",
              "      <td>1.644099</td>\n",
              "      <td>3.938026</td>\n",
              "      <td>2.474587</td>\n",
              "      <td>1.802591</td>\n",
              "      <td>1.909056</td>\n",
              "      <td>3.808333</td>\n",
              "      <td>4.387366</td>\n",
              "      <td>1.562823</td>\n",
              "      <td>1.647664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Unemployment</td>\n",
              "      <td>8.108333</td>\n",
              "      <td>RealGDP</td>\n",
              "      <td>2.99646</td>\n",
              "      <td>Core CPI</td>\n",
              "      <td>2.290573</td>\n",
              "      <td>Core PCE</td>\n",
              "      <td>1.664742</td>\n",
              "      <td>8.142979</td>\n",
              "      <td>1.312183</td>\n",
              "      <td>2.193956</td>\n",
              "      <td>1.751450</td>\n",
              "      <td>8.108333</td>\n",
              "      <td>0.570160</td>\n",
              "      <td>2.223187</td>\n",
              "      <td>1.726642</td>\n",
              "      <td>4.816343</td>\n",
              "      <td>0.651697</td>\n",
              "      <td>2.003969</td>\n",
              "      <td>2.245712</td>\n",
              "      <td>6.338333</td>\n",
              "      <td>-1.492464</td>\n",
              "      <td>2.231115</td>\n",
              "      <td>1.727602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Unemployment</td>\n",
              "      <td>4.350000</td>\n",
              "      <td>RealGDP</td>\n",
              "      <td>4.75324</td>\n",
              "      <td>Core CPI</td>\n",
              "      <td>2.290573</td>\n",
              "      <td>Core PCE</td>\n",
              "      <td>1.559324</td>\n",
              "      <td>4.346418</td>\n",
              "      <td>4.914411</td>\n",
              "      <td>2.220703</td>\n",
              "      <td>1.585213</td>\n",
              "      <td>4.350000</td>\n",
              "      <td>4.559254</td>\n",
              "      <td>2.222196</td>\n",
              "      <td>1.718909</td>\n",
              "      <td>4.817755</td>\n",
              "      <td>4.013551</td>\n",
              "      <td>2.108200</td>\n",
              "      <td>2.264820</td>\n",
              "      <td>4.350000</td>\n",
              "      <td>4.377784</td>\n",
              "      <td>2.249111</td>\n",
              "      <td>1.975634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2665</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RealGDP</td>\n",
              "      <td>2.68428</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.994133</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.939068</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.166291</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.759920</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2666</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RealGDP</td>\n",
              "      <td>3.07551</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.578147</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.686900</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.719996</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.108490</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2667</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RealGDP</td>\n",
              "      <td>4.12748</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.376538</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.127480</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.387583</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.252632</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2668</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RealGDP</td>\n",
              "      <td>2.56377</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.506146</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.401250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.320658</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.375238</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2669</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RealGDP</td>\n",
              "      <td>4.44722</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.214095</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.251551</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.467689</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.312276</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2670 rows Ã— 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         INDICATOR  ...  KNeigborsRegressor Core PCE\n",
              "0     Unemployment  ...                     1.237391\n",
              "1     Unemployment  ...                     1.439257\n",
              "2     Unemployment  ...                     1.647664\n",
              "3     Unemployment  ...                     1.727602\n",
              "4     Unemployment  ...                     1.975634\n",
              "...            ...  ...                          ...\n",
              "2665           NaN  ...                          NaN\n",
              "2666           NaN  ...                          NaN\n",
              "2667           NaN  ...                          NaN\n",
              "2668           NaN  ...                          NaN\n",
              "2669           NaN  ...                          NaN\n",
              "\n",
              "[2670 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrynsjqXC_mG"
      },
      "source": [
        "uX = df[[\"XGBoost Unemployment\", \"Random Forest Unemployment\", \"Linear Regression Unemployment\", \"KNeigborsRegressor Unemployment\"]]\n",
        "uY = df[[\"Unemployment actual\"]]\n",
        "\n",
        "uX = uX.dropna()\n",
        "uY = uY.dropna()"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h01F04ryDXO6"
      },
      "source": [
        "gX = df[[\"XGBoost RealGDP\", \"Random Forest RealGDP\", \"Linear Regression RealGDP\", \"KNeigborsRegressor RealGDP\"]]\n",
        "gY = df[[\"RealGDP actual\"]]"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9aE-DSaEEG0"
      },
      "source": [
        "cX = df[[\"XGBoost Core CPI\", \"Random Forest Core CPI\", \"Linear Regression Core CPI\", \"KNeigborsRegressor Core CPI\"]]\n",
        "cY = df[[\"Core CPI actual\"]]\n",
        "\n",
        "cX = cX.dropna()\n",
        "cY = cY.dropna()"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yslW5RuEPCA"
      },
      "source": [
        "pX = df[[\"XGBoost Core PCE\", \"Random Forest Core PCE\", \"Linear Regression Core PCE\", \"KNeigborsRegressor Core PCE\"]]\n",
        "pY = df[[\"Core PCE actual\"]]\n",
        "\n",
        "pX = pX.dropna()\n",
        "pY = pY.dropna()"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9vL3QVyEVrB"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlD7pMfJEpMH"
      },
      "source": [
        "def create_mlp(dim, regress=False):\n",
        "\t# define our MLP network\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
        "\tmodel.add(Dense(4, activation=\"relu\"))\n",
        "\t# check to see if the regression node should be added\n",
        "\tif regress:\n",
        "\t\tmodel.add(Dense(1, activation=\"linear\"))\n",
        "\t# return our model\n",
        "\treturn model"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqpfJfMHEpqz",
        "outputId": "442746bb-63f3-4e8b-b05d-90e85f0560ac"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import argparse\n",
        "import locale\n",
        "import os\n",
        "\n",
        "model = create_mlp(uX.shape[1], regress=True)\n",
        "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
        "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
        "# train the model\n",
        "print(\"[INFO] training model...\")\n",
        "model.fit(x=uX, y=uY, \n",
        "\tvalidation_data=(uX, uY),\n",
        "\tepochs=200, batch_size=8)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] training model...\n",
            "Epoch 1/200\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 6997378.2114 - val_loss: 1221735.7500\n",
            "Epoch 2/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 809068.8258 - val_loss: 544189.2500\n",
            "Epoch 3/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 275275.5538 - val_loss: 330439.0312\n",
            "Epoch 4/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 257432.1025 - val_loss: 86610.5078\n",
            "Epoch 5/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 78288.7826 - val_loss: 158498.6562\n",
            "Epoch 6/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 68911.9352 - val_loss: 157228.6562\n",
            "Epoch 7/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 111881.8609 - val_loss: 150679.5312\n",
            "Epoch 8/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 186228.0078 - val_loss: 67666.3672\n",
            "Epoch 9/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 55912.9758 - val_loss: 27270.0000\n",
            "Epoch 10/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 111012.3821 - val_loss: 55252.9297\n",
            "Epoch 11/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 47318.2692 - val_loss: 130239.9297\n",
            "Epoch 12/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 119132.7689 - val_loss: 66179.3125\n",
            "Epoch 13/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 44554.9255 - val_loss: 134020.4219\n",
            "Epoch 14/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 73904.3250 - val_loss: 24237.3477\n",
            "Epoch 15/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 52024.4065 - val_loss: 125232.0391\n",
            "Epoch 16/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 75276.0309 - val_loss: 33809.1328\n",
            "Epoch 17/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 59257.4566 - val_loss: 78056.2422\n",
            "Epoch 18/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 189023.4600 - val_loss: 42927.9297\n",
            "Epoch 19/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 69049.0041 - val_loss: 98336.3984\n",
            "Epoch 20/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 76056.7006 - val_loss: 48445.0586\n",
            "Epoch 21/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 102934.5486 - val_loss: 83873.5078\n",
            "Epoch 22/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 80423.5136 - val_loss: 34040.3125\n",
            "Epoch 23/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 98261.3118 - val_loss: 118189.7109\n",
            "Epoch 24/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 102229.5142 - val_loss: 150611.6719\n",
            "Epoch 25/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 66332.0529 - val_loss: 64772.9023\n",
            "Epoch 26/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 104373.6225 - val_loss: 167829.3750\n",
            "Epoch 27/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 59811.2109 - val_loss: 129500.0078\n",
            "Epoch 28/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 101030.0207 - val_loss: 188336.5781\n",
            "Epoch 29/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 143957.9111 - val_loss: 277216.3750\n",
            "Epoch 30/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 311137.0714 - val_loss: 114169.5625\n",
            "Epoch 31/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 83411.8355 - val_loss: 59025.7227\n",
            "Epoch 32/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 58275.3707 - val_loss: 154342.4375\n",
            "Epoch 33/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 84435.0628 - val_loss: 112704.7500\n",
            "Epoch 34/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 191895.9538 - val_loss: 84608.1875\n",
            "Epoch 35/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 71271.5604 - val_loss: 96565.0625\n",
            "Epoch 36/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 55701.8861 - val_loss: 56311.7070\n",
            "Epoch 37/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 78745.1122 - val_loss: 84393.0547\n",
            "Epoch 38/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 120034.5178 - val_loss: 76914.7188\n",
            "Epoch 39/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 117699.5370 - val_loss: 131470.6719\n",
            "Epoch 40/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 73275.5087 - val_loss: 51224.0586\n",
            "Epoch 41/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 107093.5023 - val_loss: 31145.4375\n",
            "Epoch 42/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 63881.4786 - val_loss: 68016.4062\n",
            "Epoch 43/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 57106.2843 - val_loss: 83722.7031\n",
            "Epoch 44/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 93120.0466 - val_loss: 83772.7812\n",
            "Epoch 45/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 59530.1548 - val_loss: 144834.3281\n",
            "Epoch 46/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 103027.1436 - val_loss: 36940.9023\n",
            "Epoch 47/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 122110.2593 - val_loss: 71825.4453\n",
            "Epoch 48/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 93065.7093 - val_loss: 24805.1367\n",
            "Epoch 49/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 68223.4355 - val_loss: 38424.1562\n",
            "Epoch 50/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 57294.0498 - val_loss: 176619.8281\n",
            "Epoch 51/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 78201.3647 - val_loss: 51491.0820\n",
            "Epoch 52/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 58986.4741 - val_loss: 42417.6914\n",
            "Epoch 53/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 87206.1189 - val_loss: 36001.9336\n",
            "Epoch 54/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 105794.0999 - val_loss: 98005.2891\n",
            "Epoch 55/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 128589.8466 - val_loss: 56342.1016\n",
            "Epoch 56/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 63029.2604 - val_loss: 184616.7344\n",
            "Epoch 57/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 70749.0385 - val_loss: 134741.9375\n",
            "Epoch 58/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 86815.8211 - val_loss: 42342.1211\n",
            "Epoch 59/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 54149.8615 - val_loss: 148497.3438\n",
            "Epoch 60/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 93894.8796 - val_loss: 35068.2969\n",
            "Epoch 61/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 48456.6579 - val_loss: 12659.1426\n",
            "Epoch 62/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 111046.4207 - val_loss: 116727.7422\n",
            "Epoch 63/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 66462.7690 - val_loss: 75517.0234\n",
            "Epoch 64/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 81846.2183 - val_loss: 140246.0156\n",
            "Epoch 65/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 93088.3314 - val_loss: 23771.8750\n",
            "Epoch 66/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 54814.0484 - val_loss: 41109.4297\n",
            "Epoch 67/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 43182.5690 - val_loss: 48897.6914\n",
            "Epoch 68/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 60461.1808 - val_loss: 143487.2969\n",
            "Epoch 69/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 50876.2642 - val_loss: 47626.8711\n",
            "Epoch 70/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 47694.6420 - val_loss: 59848.5938\n",
            "Epoch 71/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 94459.1376 - val_loss: 78372.5703\n",
            "Epoch 72/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 35675.9871 - val_loss: 31551.4180\n",
            "Epoch 73/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 62067.7991 - val_loss: 79119.1250\n",
            "Epoch 74/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 47001.3612 - val_loss: 101897.1562\n",
            "Epoch 75/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 97135.4623 - val_loss: 55653.3750\n",
            "Epoch 76/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 34627.2485 - val_loss: 67998.9531\n",
            "Epoch 77/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 66099.8636 - val_loss: 61173.0781\n",
            "Epoch 78/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 37044.3696 - val_loss: 77153.9453\n",
            "Epoch 79/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 63861.6220 - val_loss: 137781.7344\n",
            "Epoch 80/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 99695.9135 - val_loss: 24414.8281\n",
            "Epoch 81/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 30191.6891 - val_loss: 88077.0703\n",
            "Epoch 82/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 71377.6687 - val_loss: 98570.6484\n",
            "Epoch 83/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 54070.2816 - val_loss: 75907.9766\n",
            "Epoch 84/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 78206.1139 - val_loss: 98998.2734\n",
            "Epoch 85/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 44798.8695 - val_loss: 34247.4375\n",
            "Epoch 86/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 51598.4892 - val_loss: 59239.4062\n",
            "Epoch 87/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 47370.8533 - val_loss: 41792.9570\n",
            "Epoch 88/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 34247.1445 - val_loss: 46875.1484\n",
            "Epoch 89/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 76554.8024 - val_loss: 26213.5684\n",
            "Epoch 90/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 52384.9583 - val_loss: 15281.4990\n",
            "Epoch 91/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 31783.3606 - val_loss: 71220.9531\n",
            "Epoch 92/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 59033.7952 - val_loss: 45937.4922\n",
            "Epoch 93/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 31931.8957 - val_loss: 58495.6602\n",
            "Epoch 94/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 52912.7268 - val_loss: 11239.1201\n",
            "Epoch 95/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 32278.5860 - val_loss: 85067.1406\n",
            "Epoch 96/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 69489.4827 - val_loss: 122183.7188\n",
            "Epoch 97/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 59855.1099 - val_loss: 71664.8672\n",
            "Epoch 98/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 93722.4310 - val_loss: 23154.3965\n",
            "Epoch 99/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 64684.2443 - val_loss: 23689.2754\n",
            "Epoch 100/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 38271.0225 - val_loss: 32583.8750\n",
            "Epoch 101/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 50106.6323 - val_loss: 20975.1660\n",
            "Epoch 102/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 39302.2380 - val_loss: 47799.9883\n",
            "Epoch 103/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 77257.5436 - val_loss: 117763.8125\n",
            "Epoch 104/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 52610.7750 - val_loss: 49298.3984\n",
            "Epoch 105/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 52942.9409 - val_loss: 13646.9883\n",
            "Epoch 106/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 29128.4027 - val_loss: 26530.4570\n",
            "Epoch 107/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 53625.8056 - val_loss: 12708.2012\n",
            "Epoch 108/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 25221.2438 - val_loss: 14034.1318\n",
            "Epoch 109/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 56324.3619 - val_loss: 39514.7617\n",
            "Epoch 110/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 65431.4380 - val_loss: 29276.0586\n",
            "Epoch 111/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 51408.6826 - val_loss: 74546.2812\n",
            "Epoch 112/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 41991.9434 - val_loss: 41280.7070\n",
            "Epoch 113/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 46153.9801 - val_loss: 19833.3555\n",
            "Epoch 114/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 46809.1013 - val_loss: 62018.9102\n",
            "Epoch 115/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 61202.7620 - val_loss: 132334.6094\n",
            "Epoch 116/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 59099.6398 - val_loss: 44089.2383\n",
            "Epoch 117/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 42878.6163 - val_loss: 38935.2383\n",
            "Epoch 118/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 51456.6727 - val_loss: 39418.0508\n",
            "Epoch 119/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 47657.2940 - val_loss: 18042.5117\n",
            "Epoch 120/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 50936.6081 - val_loss: 4112.4409\n",
            "Epoch 121/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 39486.4273 - val_loss: 19716.9863\n",
            "Epoch 122/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 54627.9107 - val_loss: 47625.1484\n",
            "Epoch 123/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 67320.6008 - val_loss: 42034.0156\n",
            "Epoch 124/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 57885.6701 - val_loss: 26078.2480\n",
            "Epoch 125/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 32590.8632 - val_loss: 64141.5547\n",
            "Epoch 126/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 34279.2220 - val_loss: 46440.6992\n",
            "Epoch 127/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 84489.2028 - val_loss: 40751.2422\n",
            "Epoch 128/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 50641.5242 - val_loss: 78760.4297\n",
            "Epoch 129/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 50352.9412 - val_loss: 44028.4219\n",
            "Epoch 130/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 31078.6091 - val_loss: 5316.2407\n",
            "Epoch 131/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 55649.5452 - val_loss: 4121.8447\n",
            "Epoch 132/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 46109.8987 - val_loss: 104692.5469\n",
            "Epoch 133/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 98071.1071 - val_loss: 3656.1567\n",
            "Epoch 134/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 33156.2315 - val_loss: 63077.3281\n",
            "Epoch 135/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 57383.0019 - val_loss: 63232.8398\n",
            "Epoch 136/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 39624.1707 - val_loss: 159976.2656\n",
            "Epoch 137/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 69928.4783 - val_loss: 49737.2734\n",
            "Epoch 138/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 94838.6566 - val_loss: 62564.2305\n",
            "Epoch 139/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 58516.6638 - val_loss: 23062.3633\n",
            "Epoch 140/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 34926.4981 - val_loss: 137758.1406\n",
            "Epoch 141/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 24620.6806 - val_loss: 60451.0703\n",
            "Epoch 142/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 63751.9248 - val_loss: 14985.8848\n",
            "Epoch 143/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 40727.6942 - val_loss: 71940.6094\n",
            "Epoch 144/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 37550.2841 - val_loss: 211.7106\n",
            "Epoch 145/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 48863.7095 - val_loss: 40899.1914\n",
            "Epoch 146/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 42486.9313 - val_loss: 27697.6719\n",
            "Epoch 147/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 30610.0130 - val_loss: 9119.9619\n",
            "Epoch 148/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 40537.7553 - val_loss: 25248.5664\n",
            "Epoch 149/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 59599.2355 - val_loss: 24721.3848\n",
            "Epoch 150/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 28791.1753 - val_loss: 29297.4434\n",
            "Epoch 151/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 41766.7693 - val_loss: 50872.3555\n",
            "Epoch 152/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 47785.9473 - val_loss: 31957.7031\n",
            "Epoch 153/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 43249.6868 - val_loss: 43481.3203\n",
            "Epoch 154/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 47293.7042 - val_loss: 18965.6094\n",
            "Epoch 155/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 32263.7301 - val_loss: 4384.7451\n",
            "Epoch 156/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 30176.1987 - val_loss: 10990.4980\n",
            "Epoch 157/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 51723.6137 - val_loss: 969.3880\n",
            "Epoch 158/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 53273.0139 - val_loss: 147620.2500\n",
            "Epoch 159/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 79419.2622 - val_loss: 14749.6035\n",
            "Epoch 160/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 30312.3001 - val_loss: 33277.0078\n",
            "Epoch 161/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 43803.9677 - val_loss: 3544.4426\n",
            "Epoch 162/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 70681.5987 - val_loss: 63440.4609\n",
            "Epoch 163/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 43700.2868 - val_loss: 6501.0601\n",
            "Epoch 164/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 28737.0399 - val_loss: 136376.7656\n",
            "Epoch 165/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 52620.5197 - val_loss: 54346.2852\n",
            "Epoch 166/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 73751.9499 - val_loss: 64984.0547\n",
            "Epoch 167/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 59569.2266 - val_loss: 98310.9062\n",
            "Epoch 168/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 56438.5490 - val_loss: 706.1308\n",
            "Epoch 169/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 39045.7922 - val_loss: 62735.7188\n",
            "Epoch 170/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 31529.7940 - val_loss: 72825.9297\n",
            "Epoch 171/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 35976.9607 - val_loss: 75121.2734\n",
            "Epoch 172/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 69971.2677 - val_loss: 27747.1777\n",
            "Epoch 173/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 42035.3640 - val_loss: 118410.0781\n",
            "Epoch 174/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 48428.1236 - val_loss: 20242.4805\n",
            "Epoch 175/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 49624.7049 - val_loss: 25562.9219\n",
            "Epoch 176/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 25965.2135 - val_loss: 132107.5000\n",
            "Epoch 177/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 96183.8863 - val_loss: 46503.6367\n",
            "Epoch 178/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 58190.1645 - val_loss: 101411.6719\n",
            "Epoch 179/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 39204.3115 - val_loss: 68006.3125\n",
            "Epoch 180/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 70035.5135 - val_loss: 10573.2393\n",
            "Epoch 181/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 46386.0953 - val_loss: 75249.0703\n",
            "Epoch 182/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 42992.9056 - val_loss: 25649.5039\n",
            "Epoch 183/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 29610.7316 - val_loss: 67737.3984\n",
            "Epoch 184/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 57271.0717 - val_loss: 29637.4473\n",
            "Epoch 185/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 43477.2184 - val_loss: 49451.8984\n",
            "Epoch 186/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 48015.3267 - val_loss: 39259.5977\n",
            "Epoch 187/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 49725.7347 - val_loss: 47141.6719\n",
            "Epoch 188/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 47120.2969 - val_loss: 5268.8081\n",
            "Epoch 189/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 33153.5499 - val_loss: 13112.2324\n",
            "Epoch 190/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 49377.0208 - val_loss: 45631.1484\n",
            "Epoch 191/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 59391.9033 - val_loss: 3745.2546\n",
            "Epoch 192/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 27741.0369 - val_loss: 34505.7266\n",
            "Epoch 193/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 31312.4202 - val_loss: 16789.1445\n",
            "Epoch 194/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 28659.1799 - val_loss: 60974.0508\n",
            "Epoch 195/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 46756.3501 - val_loss: 45126.8789\n",
            "Epoch 196/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 38364.4389 - val_loss: 45150.4453\n",
            "Epoch 197/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 48705.2722 - val_loss: 74948.4297\n",
            "Epoch 198/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 55544.3838 - val_loss: 65346.7812\n",
            "Epoch 199/200\n",
            "200/200 [==============================] - 0s 2ms/step - loss: 43837.4732 - val_loss: 83028.9609\n",
            "Epoch 200/200\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 68294.2535 - val_loss: 8501.2197\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc1d6547890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpcD-NQ0Ezau"
      },
      "source": [
        "preds = model.predict(uX)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCEjZ-uCGErJ",
        "outputId": "8f990c9f-46d8-4b39-939d-656e4c792216"
      },
      "source": [
        "preds"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-5.636699e-05],\n",
              "       [-5.636699e-05],\n",
              "       [-5.636699e-05],\n",
              "       ...,\n",
              "       [-5.636699e-05],\n",
              "       [-5.636699e-05],\n",
              "       [-5.636699e-05]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVy0cEDfHzvH"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQYpRBIkH9aq",
        "outputId": "947cc4e0-3611-4b51-f7bc-285da78d0a56"
      },
      "source": [
        "reg = LinearRegression().fit(gX, gY)\n",
        "reg.score(gX, gY)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9549310708629474"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kK21dADxIFfu",
        "outputId": "b16f0855-3796-41e2-d8aa-cd56d00b88fb"
      },
      "source": [
        "reg.predict(gX)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.56188736],\n",
              "       [4.1774761 ],\n",
              "       [4.15642966],\n",
              "       ...,\n",
              "       [4.21928957],\n",
              "       [2.43410261],\n",
              "       [4.23721564]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4aVBJSyITOM",
        "outputId": "dde01e1e-3138-4c62-8f18-f28ab2e5c3a9"
      },
      "source": [
        "reg.coef_"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.39451943,  0.62355299, -0.00584316, -0.0121875 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDNW9dmMIajt",
        "outputId": "172082ec-d8d1-4c68-c4b6-df8af8f77aa7"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "pred = reg.predict(gX)\n",
        "actual = gY\n",
        "\n",
        "np.sqrt(mean_squared_error(pred, actual))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4089700273137137"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLQToPGyOH__",
        "outputId": "889d17d7-3fc3-4315-96b6-39b15547eb81"
      },
      "source": [
        "reg = LinearRegression().fit(gX, gY)\n",
        "print(reg.score(gX, gY))\n",
        "\n",
        "pred = reg.predict(gX)\n",
        "actual = gY\n",
        "\n",
        "np.sqrt(mean_squared_error(pred, actual))"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9549310708629474\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4089700273137137"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xm5bee0vNFIg",
        "outputId": "dfdb656a-0986-4568-cb00-7e7c00cae9d9"
      },
      "source": [
        "reg = LinearRegression().fit(uX, uY)\n",
        "print(reg.score(uX, uY))\n",
        "\n",
        "pred = reg.predict(uX)\n",
        "actual = uY\n",
        "\n",
        "np.sqrt(mean_squared_error(pred, actual))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9437394066880166\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6735477307884534"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUoIvhdhJCpv",
        "outputId": "b3266105-1f50-4c37-bc7e-a5e516c9237a"
      },
      "source": [
        "reg = LinearRegression().fit(cX, cY)\n",
        "print(reg.score(cX, cY))\n",
        "\n",
        "pred = reg.predict(cX)\n",
        "actual = cY\n",
        "\n",
        "np.sqrt(mean_squared_error(pred, actual))"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9251698478091955\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.33160928669688405"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPjmaFawNJWE",
        "outputId": "686715c9-69bf-41c4-8755-fd996e6b31be"
      },
      "source": [
        "reg = LinearRegression().fit(pX, pY)\n",
        "print(reg.score(pX, pY))\n",
        "\n",
        "pred = reg.predict(pX)\n",
        "actual = pY\n",
        "\n",
        "np.sqrt(mean_squared_error(pred, actual))"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9311808648943892\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.31517548684660446"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YqvHpJRNN0o"
      },
      "source": [
        ""
      ],
      "execution_count": 126,
      "outputs": []
    }
  ]
}