{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "train = pd.read_csv('..\\data\\\\TrainingData\\\\trainingWithItems.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp = train[train['INDICATOR'] == \"Unemployment\"]\n",
    "rgdp = train[train['INDICATOR'] == \"RealGDP\"]\n",
    "cpce = train[train['INDICATOR'] == \"Core PCE\"]\n",
    "ccpi = train[train['INDICATOR'] == \"Core CPI\"]\n",
    "ngnp = train[train['INDICATOR'] == \"NominalGNP\"]\n",
    "rgnp = train[train['INDICATOR'] == \"RealGNP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "def model_setup(df):\n",
    "    \n",
    "    rand = df.sample(frac = 1)\n",
    "    #80/20 split\n",
    "    nrow = len(rand)\n",
    "    train_prop = int(np.round(nrow*0.8))\n",
    "    train = rand[:train_prop]\n",
    "    val = rand[train_prop:]\n",
    "    \n",
    "    train = train[[\"FORECASTER ID\", \"MAX\", \"pred_average\", \"pred_var\", \"HIT\", \"ACTUAL_CONF\", \"actual\",\n",
    "                  \"chicken\", \"coffee\", \"eggs\", \"electricity\", \"rice\", \"unleadedGasoline\"]]\n",
    "    val = val[[\"FORECASTER ID\", \"MAX\", \"pred_average\", \"pred_var\", \"HIT\", \"ACTUAL_CONF\", \"actual\",\n",
    "              \"chicken\", \"coffee\", \"eggs\", \"electricity\", \"rice\", \"unleadedGasoline\"]]\n",
    "    \n",
    "    return train.dropna(), val.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(df, models):\n",
    "    train, val = model_setup(df)\n",
    "    X_train, Y_train = train[[\"FORECASTER ID\", \"MAX\", \"pred_average\", \"pred_var\", \"HIT\", \"ACTUAL_CONF\",\n",
    "                             \"chicken\", \"coffee\", \"eggs\", \"electricity\", \"rice\", \"unleadedGasoline\"]], train[\"actual\"].tolist()\n",
    "    X_val, Y_val = val[[\"FORECASTER ID\", \"MAX\", \"pred_average\", \"pred_var\", \"HIT\", \"ACTUAL_CONF\",\n",
    "                       \"chicken\", \"coffee\", \"eggs\", \"electricity\", \"rice\", \"unleadedGasoline\"]], val[\"actual\"].tolist()\n",
    "    # fit and evaluate the models\n",
    "    scores = []\n",
    "        \n",
    "    for name, model in models:\n",
    "        model.fit(X_train, Y_train)\n",
    "        if model == RandomForestRegressor():\n",
    "            preds = []\n",
    "            for i in range(len(Y_val)):\n",
    "                row = X_val.iloc[[i]].values.tolist()\n",
    "                pred = model.predict(row)\n",
    "                pred = pred.flatten()[0]\n",
    "                preds.append(pred)\n",
    "            scores.append(-mean_absolute_error(Y_val, preds))\n",
    "        else:\n",
    "            preds = model.predict(X_val)\n",
    "            scores.append(-mean_absolute_error(Y_val, preds))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = model_setup(unemp)\n",
    "X_train, Y_train = train[[\"FORECASTER ID\", \"MAX\", \"pred_average\", \"pred_var\", \"HIT\", \"ACTUAL_CONF\",\n",
    "                         \"chicken\", \"coffee\", \"eggs\", \"electricity\", \"rice\", \"unleadedGasoline\"]], train[\"actual\"].tolist()\n",
    "X_val, Y_val = val[[\"FORECASTER ID\", \"MAX\", \"pred_average\", \"pred_var\", \"HIT\", \"ACTUAL_CONF\",\n",
    "                   \"chicken\", \"coffee\", \"eggs\", \"electricity\", \"rice\", \"unleadedGasoline\"]], val[\"actual\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('knn', KNeighborsRegressor()))\n",
    "# models.append(('svm', SVR()))\n",
    "models.append(('lr', LinearRegression()))\n",
    "models.append(('rf', RandomForestRegressor()))\n",
    "models.append(('xgb', xgb.XGBRegressor(verbosity=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Scoring as Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Avg MAE: 0.916\n",
      ">knn: -0.925\n",
      ">lr: -1.332\n",
      ">rf: -0.257\n",
      ">xgb: -0.310\n",
      "Voting MAE: 0.628\n"
     ]
    }
   ],
   "source": [
    "unemp_scores = evaluate_models(unemp, models)\n",
    "ensemble = VotingRegressor(estimators=models, weights=unemp_scores)\n",
    "ensemble.fit(X_train, Y_train)\n",
    "# make predictions on test set\n",
    "yhat = ensemble.predict(X_val)\n",
    "# evaluate predictions\n",
    "score = mean_absolute_error(Y_val, yhat)\n",
    "print('Weighted Avg MAE: %.3f' % (score))\n",
    "# evaluate each standalone model\n",
    "for i in range(len(models)):\n",
    "    print('>%s: %.3f' % (models[i][0], unemp_scores[i]))\n",
    "# evaluate equal weighting\n",
    "ensemble = VotingRegressor(estimators=models)\n",
    "ensemble.fit(X_train, Y_train)\n",
    "yhat = ensemble.predict(X_val)\n",
    "score = mean_absolute_error(Y_val, yhat)\n",
    "print('Voting MAE: %.3f' % (score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With SVR\n",
    "\n",
    "Weighted Avg MAE: 1.292 <br>\n",
    "knn: -0.909 <br>\n",
    "svm: -2.125 <br>\n",
    "lr: -1.273 <br>\n",
    "rf: -0.213 <br>\n",
    "xgb: -0.270 <br>\n",
    "Voting MAE: 0.808"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Ranking as Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.891889366541196, -1.353660924116945, -0.24065022183619644, -0.28433062773514056]\n",
      "[2 1 4 3]\n",
      "Weighted Avg MAE: 0.423\n",
      ">knn: -0.892\n",
      ">lr: -1.354\n",
      ">rf: -0.241\n",
      ">xgb: -0.284\n",
      "Voting MAE: 0.593\n"
     ]
    }
   ],
   "source": [
    "scores = evaluate_models(unemp)\n",
    "print(scores)\n",
    "ranking = 1 + np.argsort(np.argsort(scores))\n",
    "print(ranking)\n",
    "# create the ensemble\n",
    "ensemble = VotingRegressor(estimators=models, weights=ranking)\n",
    "# fit the ensemble on the training dataset\n",
    "ensemble.fit(X_train, Y_train)\n",
    "# make predictions on test set\n",
    "yhat = ensemble.predict(X_val)\n",
    "# evaluate predictions\n",
    "score = mean_absolute_error(Y_val, yhat)\n",
    "print('Weighted Avg MAE: %.3f' % (score))\n",
    "# evaluate each standalone model\n",
    "for i in range(len(models)):\n",
    "    print('>%s: %.3f' % (models[i][0], scores[i]))\n",
    "# evaluate equal weighting\n",
    "ensemble = VotingRegressor(estimators=models)\n",
    "ensemble.fit(X_train, Y_train)\n",
    "yhat = ensemble.predict(X_val)\n",
    "score = mean_absolute_error(Y_val, yhat)\n",
    "print('Voting MAE: %.3f' % (score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With SVR\n",
    "\n",
    "Scores: [-0.9907339846925971, -2.2477322881321817, -1.3626856438899229, -0.25836087141482267, -0.3111017540999072] <br>\n",
    "Rankings: [3 1 2 5 4] <br>\n",
    "Weighted Avg MAE: 0.568 <br>\n",
    "knn: -0.991 <br>\n",
    "svm: -2.248 <br>\n",
    "lr: -1.363 <br>\n",
    "rf: -0.258 <br>\n",
    "xgb: -0.311 <br>\n",
    "Voting MAE: 0.849"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
